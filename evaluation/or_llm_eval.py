import openai
import anthropic
from dotenv import load_dotenv
import os
import re
import subprocess
import sys
import tempfile
import copy
import json
import argparse
import concurrent.futures
from itertools import zip_longest
from utils import (
    is_number_string,
    convert_to_number,
    extract_best_objective,
    extract_and_execute_python_code,
    eval_model_result
)
import httpx

# Load environment variables from .env file
load_dotenv()

# OpenAI API setup
# 优先从环境变量读取；若未设置，则回退到远端 7B 服务地址
openai_api_base_env = os.getenv("OPENAI_API_BASE")
openai_api_key_env = os.getenv("OPENAI_API_KEY")
openai_api_data = dict(
    api_key = openai_api_key_env if openai_api_key_env else os.getenv("REMOTE_OPENAI_API_KEY", "sk-no-key"),
    base_url = openai_api_base_env if openai_api_base_env else "http://3.84.131.165:8000/v1"
)  

# Anthropic API setup
anthropic_api_data = dict(
    api_key = os.getenv("CLAUDE_API_KEY"),
)

# Initialize clients
openai_client = openai.OpenAI(
    api_key=openai_api_data['api_key'],
    base_url=openai_api_data['base_url'] if openai_api_data['base_url'] else None
)

anthropic_client = anthropic.Anthropic(
    api_key=anthropic_api_data['api_key']
)

def query_llm(messages, model_name="o3-mini", temperature=0.2):
    """
    Call LLM to get response results.
    
    Args:
        messages (list): List of conversation context.
        model_name (str): LLM model name, default is "o3-mini".
        temperature (float): Controls the randomness of output, default is 0.2.

    Returns:
        str: Response content generated by the LLM.
    """
    # Check if model is Claude (Anthropic)
    if model_name.lower().startswith("claude"):
        # Convert OpenAI message format to Anthropic format
        system_message = next((m["content"] for m in messages if m["role"] == "system"), "")
        user_messages = [m["content"] for m in messages if m["role"] == "user"]
        assistant_messages = [m["content"] for m in messages if m["role"] == "assistant"]
        
        # Combine messages into a single conversation string
        conversation = system_message + "\n\n"
        for user_msg, asst_msg in zip_longest(user_messages, assistant_messages, fillvalue=None):
            if user_msg:
                conversation += f"Human: {user_msg}\n\n"
            if asst_msg:
                conversation += f"Assistant: {asst_msg}\n\n"
        
        # Add the final user message if there is one
        if len(user_messages) > len(assistant_messages):
            conversation += f"Human: {user_messages[-1]}\n\n"

        response = anthropic_client.messages.create(
            model=model_name,
            max_tokens=8192,
            temperature=temperature,
            messages=[{
                "role": "user",
                "content": conversation
            }]
        )
        return response.content[0].text
    else:
        # Use OpenAI-compatible Chat Completions API
        # Some servers require messages.content to be a list of content blocks.
        def _to_openai_blocks(msgs):
            converted = []
            for m in msgs:
                content = m.get("content", "")
                if isinstance(content, str):
                    content_blocks = [{"type": "text", "text": content}]
                elif isinstance(content, list):
                    # Assume already content blocks
                    content_blocks = content
                else:
                    # Join tuple or convert other types to string
                    content_blocks = [{"type": "text", "text": ("\n".join(content) if isinstance(content, tuple) else str(content))}]
                converted.append({"role": m.get("role", "user"), "content": content_blocks})
            return converted

        openai_messages = _to_openai_blocks(messages)
        try:
            response = openai_client.chat.completions.create(
                model=model_name,
                messages=openai_messages,
                temperature=temperature,
                extra_body={
                    "extra": {
                        "max_context": 256000,
                        "thinking": False
                    }
                },
                stream=False
            )
        except Exception as e:
            print('-------- REQUEST FAILED --------')
            print(str(e))
            print('--------------------------------')
            # Fallback: direct HTTP call for vLLM/OpenAI-compatible servers
            try:
                headers = {"Content-Type": "application/json"}
                if openai_api_data.get('api_key'):
                    headers["Authorization"] = f"Bearer {openai_api_data['api_key']}"

                # Convert messages to simple string content for compatibility
                plain_messages = []
                for m in messages:
                    content = m.get("content", "")
                    if isinstance(content, str):
                        c = content
                    elif isinstance(content, list):
                        c = "\n".join([blk.get("text", "") if isinstance(blk, dict) else str(blk) for blk in content])
                    elif isinstance(content, tuple):
                        c = "\n".join(content)
                    else:
                        c = str(content)
                    plain_messages.append({"role": m.get("role", "user"), "content": c})

                payload = {"model": model_name, "messages": plain_messages, "temperature": temperature}
                chat_url = (openai_api_data['base_url'] or '').rstrip('/') + '/chat/completions'
                resp = httpx.post(chat_url, headers=headers, json=payload, timeout=60)
                if resp.status_code != 200:
                    raise RuntimeError(f"HTTP fallback failed: {resp.status_code} - {resp.text}")
                data = resp.json()
                msg = data.get('choices', [{}])[0].get('message', {})
                content = msg.get('content')
                if isinstance(content, list):
                    return "\n".join([b.get("text", "") for b in content if isinstance(b, dict)])
                return content
            except Exception as e2:
                print('-------- HTTP FALLBACK FAILED --------')
                print(str(e2))
                print('--------------------------------------')
                raise e
        # New SDK returns choices[0].message; content may be list of blocks or string
        message = response.choices[0].message
        if isinstance(message.content, list):
            # Concatenate text blocks
            return "\n".join([b.get("text", "") for b in message.content if isinstance(b, dict)])
        return message.content

def generate_or_code_solver(messages_bak, model_name, max_attempts, cot_ctx=None):
    """
    Generate OR code via LLM and execute with iterative fixes.

    If cot_ctx is provided, perform up to max_attempts correctness-oriented retries, where each
    attempt rebuilds messages with an updated hint based on the last execution result vs ground truth.

    Args:
        messages_bak (list): Base conversation messages (used in non-COT mode).
        model_name (str): LLM model name.
        max_attempts (int): Max attempts for generation/fix.
        cot_ctx (dict|None): Optional context for COT retries, keys:
            - user_question (str)
            - math_model (str|None)
            - answer (str|float)
            - prev_code (str|None)

    Returns:
        tuple: (success: bool, exec_result: any, messages/history, gurobi_code: str)
    """
    if cot_ctx is None:
        # Original behavior: fix based on execution errors
        messages = copy.deepcopy(messages_bak)

        gurobi_code = query_llm(messages, model_name, temperature = 0.5)
        print("【Python Gurobi Code】:\n", gurobi_code)

        text = f"{gurobi_code}"
        attempt = 0
        while attempt < max_attempts:
            try:
                success, exec_result = extract_and_execute_python_code(text)
            except Exception as e:
                success, exec_result = False, f"Exception during execution: {e}"
            if success:
                messages_bak.append({"role": "assistant", "content": gurobi_code})
                return True, exec_result, messages_bak, gurobi_code

            print(f"\nAttempt {attempt + 1} failed, requesting LLM to fix code...\n")

            # Build repair request with error message
            messages.append({"role": "assistant", "content": gurobi_code})
            messages.append({"role": "user", "content": (
                f"Code execution encountered an error, error message is as follows:\n{exec_result}\n"
                "Please fix the code and provide the complete executable code again."
            )})

            # Get the fixed code
            gurobi_code = query_llm(messages, model_name)
            text = f"{gurobi_code}"

            print("\nReceived fixed code, preparing to execute again...\n")
            attempt += 1
        # not add gurobi code
        messages_bak.append({"role": "assistant", "content": gurobi_code})
        print(f"Reached maximum number of attempts ({max_attempts}), could not execute code successfully.")
        return False, None, messages_bak, gurobi_code

    # COT-based correctness-oriented retries with dynamic hints
    user_question = cot_ctx.get('user_question')
    math_model = cot_ctx.get('math_model')
    answer = cot_ctx.get('answer')
    prev_code = cot_ctx.get('prev_code')

    attempt = 0
    last_exec_result = None
    last_success = False
    last_code = prev_code

    while attempt < max_attempts:
        hint_lines = []
        if attempt == 0:
            # First attempt, optionally include previous code as context without hint
            pass
        else:
            # Build hint based on last execution result
            if last_success and is_number_string(str(last_exec_result)) and is_number_string(str(answer)):
                obj_val = float(convert_to_number(str(last_exec_result)))
                ans_val = float(convert_to_number(str(answer)))
                if obj_val < ans_val:
                    hint_lines.append(f"previous model found {obj_val}, which is smaller than the ground truth optimal solution. If it is a minimization problem, the math model may not formulate correctly hence the model solution doesn't satisfy all constraints.  If it is a maximization problem, the math model may not formulate correctly hence the feasible region is too restricted. Also double check each term of the objective function step by step.")
                elif obj_val > ans_val:
                    hint_lines.append(f"previous model found {obj_val}, which is larger than the ground truth optimal solution. If it is a maximization problem, the math model may not formulate correctly hence the model solution doesn't satisfy all constraints.  If it is a minimization problem, the math model may not formulate correctly hence the feasible region is too restricted. Also double check each term of the objective function step by step.")
                else:
                    # Exactly matched; should have exited earlier, but handle defensively
                    pass
            elif last_success:
                hint_lines.append("The previous model found no optimal solution or could not parse the objective value.")
            else:
                hint_lines.append(f"Code execution error: {last_exec_result}")

        # Rebuild messages per attempt
        messages = [
            {"role": "system", "content": (
                "You are an operations research expert. Based on the provided question and feedback, regenerate mathematical model and executable Python code gurobipy to solve the problem. "
                "Please reason step by step to ensure the math model is correct and satisfies all constraints. ",
                "Ensure the code prints 'Optimal objective' following format 'Optimal objective\\s+([\\d.e+-]+)'. Output only the code block."
            )}
        ]
        user_content = [
            f"Question:\n{user_question}",
            f"Mathematical model:\n{math_model}" if math_model else "Mathematical model: N/A",
        ]
        if last_code:
            user_content.append(f"Previous code:\n```python\n{last_code}\n```")
        if hint_lines:
            user_content.append("Hint:\n" + "\n".join(hint_lines))
        user_content.append("Please regenerate correct and complete mathmatical model and Python code step by step. Please output only a Python code block.")
        messages.append({"role": "user", "content": "\n\n".join(user_content)})

        gurobi_code = query_llm(messages, model_name)
        print("【Python Gurobi Code】:\n", gurobi_code)
        text = f"{gurobi_code}"

        try:
            success, exec_result = extract_and_execute_python_code(text)
        except Exception as e:
            success, exec_result = False, f"Exception during execution: {e}"
        last_success = success
        last_exec_result = exec_result
        last_code = gurobi_code

        if success:
            # If matches ground truth, stop; else continue with hint
            pass_flag, correct_flag = eval_model_result(True, exec_result, answer)
            if correct_flag:
                messages_bak.append({"role": "assistant", "content": gurobi_code})
                return True, exec_result, messages_bak, gurobi_code

        print(f"\nAttempt {attempt + 1} completed, correctness not met. Updating hint and retrying...\n")
        attempt += 1

    # Attempts exhausted; return last outcome
    messages_bak.append({"role": "assistant", "content": last_code if last_code else ""})
    print(f"Reached maximum number of attempts ({max_attempts}), correctness not met.")
    return last_success, last_exec_result, messages_bak, last_code

def or_llm_agent(user_question, model_name="o3-mini", max_attempts=3):
    """
    Request Gurobi code solution from LLM and execute it, attempt to fix if it fails.

    Args:
        user_question (str): User's problem description.
        model_name (str): LLM model name to use, default is "gpt-4".
        max_attempts (int): Maximum number of attempts, default is 3.

    Returns:
        tuple: (success: bool, best_objective: float or None, final_code: str)
    """
    # Initialize conversation history
    messages = [
        {"role": "system", "content": (
            "You are an operations research expert. Based on the optimization problem provided by the user, construct a mathematical model step by step.",
            "Detail the model construction process, including variables, constraints, and the objective function. Double check each variable shall be defined as linear or integer. Rounding a coutinous variable to integer is not allowed.",
            "Double check if any cost component is missing in the objective function. If there are constant numbers in the objective function, include those so that the objective function is complete.",
            "This model will be used later to guide the generation of gurobipy code, and this step is mainly used to generate effective expressions."
        )},
        {"role": "user", "content": user_question}
    ]

    # 1. Generate mathematical model
    math_model = query_llm(messages, model_name)
    print("【Mathematical Model】:\n", math_model)

    # # 2. Validate mathematical model
    # messages.append({"role": "assistant", "content": math_model})
    # messages.append({"role": "user", "content": (
    #     "Please check if the above mathematical model matches the problem description. If there are errors, make corrections; if there are no errors, check if it can be optimized."
    #     "In any case, please output the final mathematical model again."
    # )})

    # validate_math_model = query_llm(messages, model_name)
    # print("【Validated Mathematical Model】:\n", validate_math_model)
    
    validate_math_model = math_model
    messages.append({"role": "assistant", "content": validate_math_model})
    
    # ------------------------------
    messages.append({"role": "user", "content": (
        "Based on the above mathematical model, write complete and reliable Python code using gurobipy to solve this operations research optimization problem."
        "The code should include necessary model construction, variable definitions, constraint additions, objective function settings, as well as solving and result output(if optimal solution is found, print 'Optimal objective' following format 'Optimal objective\s+([\d.e+-]+)')."
        "Output in the format ```python\n{code}\n```, without code explanations.",
        "After solving the model, print out component values of the objective function, values of decision variables, and constraints feasibility details."
    )})
    # copy msg; solve; add the laset gurobi code 
    is_solve_success, result, messages, gurobi_code = generate_or_code_solver(messages, model_name,max_attempts)
    print(f'Stage result: {is_solve_success}, {result}')
    if is_solve_success:
        if not is_number_string(result):
            print('!![No available solution warning]!!')
            # no solution 
            messages.append({"role": "user", "content": (
                "The current model resulted in *no feasible solution*. Please carefully check the mathematical model and Gurobi code for errors that might be causing the infeasibility."
                "After checking, please reoutput the Gurobi Python code."
                "Output in the format ```python\n{code}\n```, without code explanations."
            )})
            is_solve_success, result, messages, gurobi_code = generate_or_code_solver(messages, model_name, max_attempts=1)
    else:
        print('!![Max attempt debug error warning]!!')
        messages.append({"role": "user", "content": (
                "The model code still reports errors after multiple debugging attempts. Please carefully check if there are errors in the mathematical model."
                "After checking, please rebuild the Gurobi Python code."
                "Output in the format ```python\n{code}\n```, without code explanations."
            )})
        is_solve_success, result, messages, gurobi_code = generate_or_code_solver(messages, model_name, max_attempts=2)
    
    return is_solve_success, result, math_model, gurobi_code

def gpt_code_agent_simple(user_question, model_name="o3-mini", max_attempts=3):
    """
    Request Gurobi code solution from LLM and execute it, attempt to fix if it fails.

    Args:
        user_question (str): User's problem description.
        model_name (str): LLM model name to use, default is "gpt-4".
        max_attempts (int): Maximum number of attempts, default is 3.

    Returns:
        tuple: (success: bool, best_objective: float or None, final_code: str)
    """
    # Initialize conversation history
    messages = [
        {"role": "system", "content": (
            "You are an operations research expert. Based on the optimization problem provided by the user, construct a mathematical model and write complete, reliable Python code using scipy.optimize.milp to solve the operations research optimization problem."
            "The code should include necessary model construction, variable definitions, constraint additions, objective function settings, as well as solving and result output (if optimal solution is found, print 'Optimal objective' following format 'Optimal objective\s+([\d.e+-]+)')."
                "Output in the format ```python\n{code}\n```, without code explanations."
        )},
        {"role": "user", "content": user_question}
    ]

    # copy msg; solve; add the laset gurobi code
    gurobi_code = query_llm(messages, model_name)
    print("【Python Gurobi Code】:\n", gurobi_code)
    text = f"{gurobi_code}"
    try:
        is_solve_success, result = extract_and_execute_python_code(text)
    except Exception as e:
        is_solve_success, result = False, f"Exception during execution: {e}"
    
    print(f'Stage result: {is_solve_success}, {result}')
    
    return is_solve_success, result, gurobi_code

def parse_args():
    """
    Parse command line arguments.
    
    Returns:
        argparse.Namespace: The parsed arguments
    """
    parser = argparse.ArgumentParser(description='Run optimization problem solving with LLMs')
    parser.add_argument('--agent', action='store_true', default=True,
                        help='Use the agent. If not specified, directly use the model to solve the problem')
    parser.add_argument('--model', type=str, default= 'qwen-coder-ppo-lora',  # 远端 7B 模型标识，可通过 /v1/models 查询,Qwen/Qwen2.5-Coder-7B-Instruct
                        help='Model name to use for LLM queries. Use "claude-..." for Claude models.')
    parser.add_argument('--data_path', type=str, default='data/datasets/BWOR.json', #'data/datasets/IndustryOR.json',
                        help='Path to the dataset JSON file (supports both JSONL and regular JSON formats)')
    parser.add_argument('--records_out', type=str, default='data/results/QwenCoder7BPPO/records.jsonl',
                        help='Path to write per-question evaluation records (JSONL)')
    parser.add_argument('--cot_out', type=str, default='data/results/QwenCoder7BPPO/cot.jsonl',
                        help='Path to append COT entries when result equals answer (JSONL)')
    parser.add_argument('--max_workers', type=int, default=3,
                        help='Client-side concurrency (threads) for evaluation')
    return parser.parse_args()

def load_dataset(data_path):
    """
    Load dataset from either JSONL format (IndustryOR.json, BWOR.json) or regular JSON format
    """
    dataset = {}
    
    with open(data_path, 'r', encoding='utf-8') as f:
        # Try to detect format by reading first line
        first_line = f.readline().strip()
        f.seek(0)  # Reset file pointer
        
        if first_line.startswith('{"en_question"') or first_line.startswith('{"cn_question"'):
            # JSONL format (IndustryOR.json, BWOR.json)
            for line_num, line in enumerate(f, 1):
                line = line.strip()
                if line:
                    try:
                        item = json.loads(line)
                        # Convert to expected format
                        dataset_item = {
                            'question': item.get('en_question', item.get('cn_question', '')),
                            'answer': item.get('en_answer', item.get('cn_answer', '')),
                            'difficulty': item.get('difficulty', 'Unknown'),
                            'id': item.get('id', line_num - 1)
                        }
                        # Use id as string key
                        dataset[str(dataset_item['id'])] = dataset_item
                    except json.JSONDecodeError as e:
                        print(f"Warning: Could not parse line {line_num}: {line}")
                        continue
        else:
            # Regular JSON format (legacy)
            dataset = json.load(f)
    
    return dataset

def append_jsonl(path, obj):
    os.makedirs(os.path.dirname(path), exist_ok=True)
    with open(path, 'a', encoding='utf-8') as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def read_jsonl_questions(path):
    """Read previously processed user questions from a JSONL records file."""
    questions = set()
    try:
        with open(path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except Exception:
                    continue
                q = obj.get('user_question')
                if q:
                    questions.add(q)
    except FileNotFoundError:
        # No records yet
        pass
    return questions

def retry_with_cot(user_question, math_model, gurobi_code, answer, model_name, max_attempts=3):
    cot_ctx = {
        'user_question': user_question,
        'math_model': math_model,
        'answer': answer,
        'prev_code': gurobi_code,
    }
    # messages_bak is not needed in COT mode; pass empty list
    return generate_or_code_solver([], model_name, max_attempts, cot_ctx=cot_ctx)

if __name__ == "__main__":
    args = parse_args()
    
    dataset = load_dataset(args.data_path)
    #print(dataset['0'])

    model_name = args.model
    records_out_path = args.records_out
    cot_out_path = args.cot_out
    # Log selected configuration to help diagnose output paths
    print(f"[Config] model: {model_name}")
    print(f"[Config] records_out: {records_out_path}")
    print(f"[Config] cot_out: {cot_out_path}")
    print(f"[Config] max_workers: {getattr(args, 'max_workers', 1)}")

    pass_count = 0
    correct_count = 0
    error_datas = []

    def _process_item(item):
        i, d = item
        print(f"=============== num {i} ==================")
        user_question, answer = d['question'], d['answer']
        print(user_question)
        print('-------------')

        if args.agent:
            is_solve_success, llm_result, math_model, gurobi_code = or_llm_agent(user_question, model_name)
        else:
            is_solve_success, llm_result, gurobi_code = gpt_code_agent_simple(user_question, model_name)
            math_model = None

        if is_solve_success:
            print(f"Successfully executed code, optimal solution value: {llm_result}")
        else:
            print("Failed to execute code.")
        print('------------------')
        pass_flag, correct_flag = eval_model_result(is_solve_success, llm_result, answer)

        # retry_used = False
        # retry_success = False
        # retry_result = None
        # retry_code = None
        # final_is_solve_success = is_solve_success
        # final_llm_result = llm_result
        # final_gurobi_code = gurobi_code

        # if not correct_flag:
        #     print('[Retry] 重新发送 COT 到 LLM 以重新生成代码（最多三次）。')
        #     retry_used = True
        #     r_success, r_result, _, r_code = retry_with_cot(user_question, math_model, gurobi_code, answer, model_name, max_attempts=3)
        #     retry_success = r_success
        #     retry_result = r_result
        #     retry_code = r_code
        #     final_is_solve_success = r_success
        #     final_llm_result = r_result
        #     final_gurobi_code = r_code if r_code else gurobi_code

        # final_pass_flag, final_correct_flag = eval_model_result(final_is_solve_success, final_llm_result, answer)

        # print(f'solve: {final_is_solve_success}, llm: {final_llm_result}, ground truth: {answer}')
        # print(f'[Final] run pass: {final_pass_flag}, solve correct: {final_correct_flag}')
        # print(' ')

        # cot_entry = None
        # if final_correct_flag:
        #     cot_entry = {
        #         'en_question': user_question,
        #         'en_answer': str(answer),
        #         'math_model': math_model,
        #         'gurobi_code': final_gurobi_code,
        #     }

        cot_entry = None
        if correct_flag:
            cot_entry = {
                'en_question': user_question,
                'en_answer': str(answer),
                'math_model': math_model,
                'gurobi_code': gurobi_code,
            }

        # record = {
        #     'id': d.get('id', i),
        #     'difficulty': d.get('difficulty', 'Unknown'),
        #     'model': model_name,
        #     'user_question': user_question,
        #     'math_model': math_model,
        #     'gurobi_code': final_gurobi_code,
        #     'result': str(final_llm_result) if final_llm_result is not None else None,
        #     'answer': str(answer),
        #     'initial_result': str(llm_result) if llm_result is not None else None,
        #     'pass_flag': pass_flag,
        #     'correct_flag': correct_flag,
        #     'retry_used': retry_used,
        #     'retry_success': retry_success,
        #     'retry_result': str(retry_result) if retry_result is not None else None,
        #     'final_result': str(final_llm_result) if final_llm_result is not None else None,
        #     'final_pass_flag': final_pass_flag,
        #     'final_correct_flag': final_correct_flag,
        # }

        record = {
            'id': d.get('id', i),
            'difficulty': d.get('difficulty', 'Unknown'),
            'model': model_name,
            'user_question': user_question,
            'math_model': math_model,
            'gurobi_code': gurobi_code,
            'result': str(llm_result) if llm_result is not None else None,
            'answer': str(answer),
            'initial_result': str(llm_result) if llm_result is not None else None,
            'pass_flag': pass_flag,
            'correct_flag': correct_flag
        }

        return {
            'i': i,
            'pass_flag': pass_flag,
            'correct_flag': correct_flag,
            'cot_entry': cot_entry,
            'record': record,
        }

    # Skip items already recorded based on exact user_question match
    processed_questions = read_jsonl_questions(records_out_path)
    items_to_process = []
    for i, d in dataset.items():
        q_str = d.get('question', '')
        if q_str in processed_questions:
            print(f"[Skip] 题目已在 {records_out_path} 中，跳过：{q_str[:80]}...")
            continue
        items_to_process.append((i, d))
    processed_total = len(items_to_process)

    # Allow controlling client-side concurrency via CLI arg
    with concurrent.futures.ThreadPoolExecutor(max_workers=getattr(args, 'max_workers', 1)) as executor:
        # Map futures to input items so we can recover on exception
        future_to_item = {executor.submit(_process_item, item): item for item in items_to_process}
        for fut in concurrent.futures.as_completed(future_to_item):
            item = future_to_item[fut]
            try:
                res = fut.result()
            except Exception as e:
                # Gracefully handle per-item failures without stopping the whole run
                i = item[0] if isinstance(item, tuple) else None
                print(f"[Error] 题目 {i} 处理异常: {e}")
                error_datas.append(i if i is not None else "unknown")
                # Write a minimal record so downstream accounting remains consistent
                record = {
                    "i": i,
                    "question": (item[1].get('question', '') if isinstance(item, tuple) else None),
                    "pass_flag": False,
                    "correct_flag": False,
                    "exception": str(e),
                }
                append_jsonl(records_out_path, record)
                continue

            pass_count += 1 if res['pass_flag'] else 0
            correct_count += 1 if res['correct_flag'] else 0
            if not res['pass_flag'] or not res['correct_flag']:
                error_datas.append(res['i'])

            if res['cot_entry'] is not None:
                append_jsonl(cot_out_path, res['cot_entry'])
            append_jsonl(records_out_path, res['record'])

    print(f'[Total {processed_total}] run pass: {pass_count}, solve correct: {correct_count}')
    print(f'[Total fails {len(error_datas)}] error datas: {error_datas}')